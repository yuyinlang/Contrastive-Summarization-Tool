{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "With this notebook, you can obtain both positive and negative summarizations with Amazon product review input. To obtain better result, it is recommended to input at least 10 product reviews."
      ],
      "metadata": {
        "id": "Wqn0K1ACM_dh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAwImVcPM3WT",
        "outputId": "2427b9f7-0826-4705-baf6-6d9c6726f8cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# This notebook is based on Google Colab. Please follow the important instructions\n",
        "# in the notebook to avoid bugs.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install SentencePiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxF54Ts1PIJr",
        "outputId": "8e331e78-e7a5-48f0-ba71-3b0a83f0f582"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download SUM-QE\n",
        "# Note: If you do not use colab pro and run out of RAM at SUM-QE, please comment\n",
        "# the following if you rerun the whole notebook\n",
        "!git clone https://github.com/nlpaueb/SumQE.git\n",
        "!wget https://archive.org/download/sum-qe/BERT_DUC_all_Q1_Multi%20Task-1.h5\n",
        "!wget https://archive.org/download/sum-qe/BERT_DUC_all_Q2_Multi%20Task-1.h5\n",
        "!wget https://archive.org/download/sum-qe/BERT_DUC_all_Q3_Multi%20Task-1.h5\n",
        "!wget https://archive.org/download/sum-qe/BERT_DUC_all_Q4_Multi%20Task-1.h5\n",
        "!wget https://archive.org/download/sum-qe/BERT_DUC_all_Q5_Multi%20Task-1.h5"
      ],
      "metadata": {
        "id": "Y9M29YAwdL9H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip uninstall -y h5py\n",
        "!pip install tensorflow==1.15\n",
        "!pip install h5py==2.10.0\n",
        "!pip install -r SumQE/requirements.txt\n",
        "!pip install keras==2.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_1Kr75OdSdm",
        "outputId": "fe279024-8e8e-41de-ed48-75f5ce7267b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 1.15.0\n",
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "Found existing installation: h5py 2.10.0\n",
            "Uninstalling h5py-2.10.0:\n",
            "  Successfully uninstalled h5py-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15\n",
            "  Using cached tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (2.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.49.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.21.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Collecting h5py\n",
            "  Using cached h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.1.1)\n",
            "Installing collected packages: h5py, tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-3.7.0 tensorflow-1.15.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.21.6)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.7.0\n",
            "    Uninstalling h5py-3.7.0:\n",
            "      Successfully uninstalled h5py-3.7.0\n",
            "Successfully installed h5py-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto==2.49.0\n",
            "  Using cached boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Collecting boto3==1.9.228\n",
            "  Using cached boto3-1.9.228-py2.py3-none-any.whl (128 kB)\n",
            "Collecting botocore==1.12.228\n",
            "  Using cached botocore-1.12.228-py2.py3-none-any.whl (5.7 MB)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from -r SumQE/requirements.txt (line 4)) (3.0.4)\n",
            "Collecting cycler==0.10.0\n",
            "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting decorator==4.4.0\n",
            "  Using cached decorator-4.4.0-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting docutils==0.15.2\n",
            "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
            "Collecting future==0.17.1\n",
            "  Using cached future-0.17.1.tar.gz (829 kB)\n",
            "Collecting gensim==3.8.0\n",
            "  Using cached gensim-3.8.0-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Requirement already satisfied: hyperopt==0.1.2 in /usr/local/lib/python3.7/dist-packages (from -r SumQE/requirements.txt (line 10)) (0.1.2)\n",
            "Collecting idna==2.8\n",
            "  Using cached idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "Collecting jmespath==0.9.4\n",
            "  Using cached jmespath-0.9.4-py2.py3-none-any.whl (24 kB)\n",
            "Collecting Keras==2.2.5\n",
            "  Using cached Keras-2.2.5-py2.py3-none-any.whl (336 kB)\n",
            "Collecting kiwisolver==1.1.0\n",
            "  Using cached kiwisolver-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (90 kB)\n",
            "Collecting matplotlib==3.1.1\n",
            "  Using cached matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "Collecting networkx==2.3\n",
            "  Using cached networkx-2.3.zip (1.7 MB)\n",
            "Collecting nltk==3.6.5\n",
            "  Using cached nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
            "Collecting Pillow==8.3.2\n",
            "  Using cached Pillow-8.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Collecting pymagnitude==0.1.120\n",
            "  Using cached pymagnitude-0.1.120.tar.gz (5.4 MB)\n",
            "Collecting pymongo==3.9.0\n",
            "  Using cached pymongo-3.9.0-cp37-cp37m-manylinux1_x86_64.whl (447 kB)\n",
            "Collecting pyparsing==2.4.2\n",
            "  Using cached pyparsing-2.4.2-py2.py3-none-any.whl (65 kB)\n",
            "Collecting python-dateutil==2.8.0\n",
            "  Using cached python_dateutil-2.8.0-py2.py3-none-any.whl (226 kB)\n",
            "Collecting pytorch-pretrained-bert==0.6.2\n",
            "  Using cached pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "Collecting PyYAML==5.4\n",
            "  Using cached PyYAML-5.4-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "Collecting regex==2019.8.19\n",
            "  Using cached regex-2019.08.19.tar.gz (654 kB)\n",
            "Collecting requests==2.22.0\n",
            "  Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
            "Collecting s3transfer==0.2.1\n",
            "  Using cached s3transfer-0.2.1-py2.py3-none-any.whl (70 kB)\n",
            "Collecting smart-open==1.8.4\n",
            "  Using cached smart_open-1.8.4.tar.gz (63 kB)\n",
            "Collecting tensorflow-hub==0.6.0\n",
            "  Using cached tensorflow_hub-0.6.0-py2.py3-none-any.whl (84 kB)\n",
            "Collecting torch==1.2.0\n",
            "  Using cached torch-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (748.9 MB)\n",
            "Collecting torchvision==0.4.0\n",
            "  Using cached torchvision-0.4.0-cp37-cp37m-manylinux1_x86_64.whl (8.8 MB)\n",
            "Collecting tqdm==4.35.0\n",
            "  Using cached tqdm-4.35.0-py2.py3-none-any.whl (50 kB)\n",
            "Collecting urllib3==1.26.5\n",
            "  Using cached urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n",
            "INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of boto to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install -r SumQE/requirements.txt (line 3) and urllib3==1.26.5 because these package versions have conflicting dependencies.\u001b[0m\n",
            "\n",
            "The conflict is caused by:\n",
            "    The user requested urllib3==1.26.5\n",
            "    botocore 1.12.228 depends on urllib3<1.26 and >=1.20; python_version >= \"3.4\"\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.7/dist-packages (2.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (6.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import PegasusForConditionalGeneration, AutoTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmZTeLErOIwe",
        "outputId": "0672bcc4-dcf7-48db-fa37-058f9748479c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please enter the reviews here as string \n",
        "# Recommendation: To achieve the best result, please enter at least 10 reviews.\n",
        "# Recommendation: Please pick both high rating and low rating reviews\n",
        "\n",
        "reviews = []\n",
        "reviews.append(\"Professional 2200W high power, it is very strong, and it blows quickly. it has 3 temperature settings, 2 fan speeds and a cool shot button. This is my second Trezoro and I really like it because it is the only hair dryer that seems to prevent a lot of static.\")\n",
        "reviews.append(\"After two months my product broke. The plug for the hairdryer had an electric issue and short wired cause sparks to fly and for my hair dryer to break. I called Amazon and they assured me I had a one year warranty and gave me the email to contact Trezoro. I emailed them 3 different times in the span of a week and they have ignored me. Not okay! Treat your customers appropriately! For a 50 dollar hair dryer it should last longer and you should follow through on your warranty!\")\n",
        "reviews.append(\"I love that this blow dried my hair in minutes. Doesn’t make my hair frizzy and Greta hold when styling it. The only thing I wish it was a little shorter in length and lighter. It’s very tiring. Overall it’s a great dryer!!\")\n",
        "reviews.append(\"This is the worst hairdryer that I've ever used. It took forever to dry my hair and it caused so much frizz! I only bought it to use with my black orchid diffuser and I regret wasting my money on it. Its only positive feature is that it fits this diffuser. I'm glad to say that I did find a great dryer, the conair Infinitipro 1875 (the cheap rust orange one) and this one does dry my hair rapidly and I have far less frizz with it. While it didn't fit my black orchid, it can be adjusted with a snozzlepro or it can fit my adjustable segbeauty diffuser. I wish I had bought that dryer from the beginning instead. It cost me less than half of what I originally spent on the trezoro (I got it for $12.99 at Ross so it was even cheaper than it goes on for here) and it performs like a high end salon dryer. If anyone is looking for a dryer to use with a diffuser like the black orchid or segbeauty get that or a conair dryer instead. Again it can be adjusted with a snozzlepro or removing the inner ring from inside the diffuser.\")\n",
        "reviews.append(\"I had this product one year, and was satisfied with its ability to occasionally dry my hair. After just over a year though I went to plug in the dryer and screws fell out in my hand and the plastic crumbled. It left the reset switch inside the plug, and wires exposed. I had no choice but to throw it away. Not worth the risk of keeping around the house with kids at home. Glad it fell apart before I plugged it in, and not after. Terrible quality. Buyer beware. I wouldn’t trust this brand again. The hunt for a new hair dryer continues!\")\n",
        "reviews.append(\"I purchased this hairdryer after borrowing it from a friend and LOVING it! I purchased in December of 2020. This morning (06/09/2022) as I was using it, I noticed that the cord was getting extremely hot. In fact burning my skin when it came into contact with it. I thought that was ver strange because it take about 7 minutes to blow dry my hair and I was only 1/2 way finished. Seconds later black smoke started rolling out where the cord attaches to the device. I could hear the wires sizzling and the smell of an electrical fire filled the bathroom. Of course, I immediately unplugged it and quit using it. I followed up with my friend who recommended it and she said she had issues with hers to around the two year mark. I would be cautious purchasing this. I do plan to contact the company (if I can find their contact information).\")\n",
        "reviews.append(\"Purchased this blow dryer after researching. This one seemed to check all of the boxes. When it arrived, it did NOT come with any styling attachment as mentioned in the description. I tried to use if for the first time and the plug got EXTREMELY hot. After looking at reviews again, it seems I am not the only one to experience this issue. Happy I realized quickly before something worse happened.\")\n",
        "reviews.append(\"First, I'd like to say this is one of the best hair dryers I've ever had. Second it definitely very quiet! It drys my hair super fast BUT it's really NOT for me for the following reasons. I have very fine and thin hair..with that in mind the dryer has only 2 speeds 'High' and 'Medium High' so, it blows my hair everywhere and because it's fine it tangles easier. Also, it makes it extremely challenging to style my hair while drying. It has 3 heat settings so why not 3 speeds? If you have alot of hair, or thick hair this dryer is great. It us NOT 'Travel' size...it's the same size as every other dryer on the market but it feels great with the silicone covering and its light weight. I'm keeping it to use on my dogs because they have thick hair and I bathe them often and it works great for them. If they made one with a lower speed you'd have to go along way to find this quality for the price.\")\n",
        "reviews.append(\"This is the second one of these hair dryers that I have purchased. I love the dryer because it dries my thick curly hair quickly and smoothly. It gets hot with a lot of air volume, which is what I need to smooth out my hair quickly. The only downside is that the dryer lasted just two years before it burned out. And maybe that is the price to pay for a dryer that does get pretty hot. But I liked it well enough to buy another one!\")\n",
        "reviews.append(\"The main thing is that the hair dryer is easy to use, feels good in my hand, and gets the job done as advertised. Oh one more little suggestion— I use the nozzle attachment, and when I switch sides, I need to rotate it a little bit so the airflow is pointing at the correct angle. Due to the awesome matte finish, it’s kinda hard to rotate the attachment so I end up having to get a better hold of it to rotate, which tends to burn my hand a little. Perhaps a band of smoother material in just that area would make it easier to change direction. On the other hand, it’s nice that it retains its position and doesn’t easily get knocked out of place or fall of when I put it away in the drawer. Also, not important enough to change my review or overall opinion, just a little feedback for your design team!\")"
      ],
      "metadata": {
        "id": "Wm8G2YUCQQhS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'reviews': reviews}\n",
        "df_raw_reviews = pd.DataFrame(df_dict)"
      ],
      "metadata": {
        "id": "s-Ag-fCHXUag"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "# Please enter the path of models here\n",
        "bert_path = '/content/drive/MyDrive/GR/saved/finetuned_BERT_large_uncased_0.983_0.966.model'\n",
        "\n",
        "#If you predownload SUM-QE models, please also enter the path of the models here\n",
        "sum_qe_path = ''\n",
        "\n",
        "# After preprocessing the reviews, we save the classified sentences (positive and negative) here\n",
        "# Summarization is also saved here\n",
        "save_path = '/content/drive/MyDrive/GR/classified review'\n",
        "\n",
        "# Set confidence limit here, the default value is 0.8\n",
        "# The higher this limit is, the more accurate the BERT classifier is\n",
        "confidence_limit = 0.8"
      ],
      "metadata": {
        "id": "zIE6QHjBOPVi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', \n",
        "                                          do_lower_case=True)\n",
        "\n",
        "label_dict = {'neu': 0, 'pos': 1, 'neg': 2}\n",
        "label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
        "\n",
        "# GPU is needed here. The free GPU of Google Colab would be enough.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\",\n",
        "                                                       num_labels = len(label_dict),\n",
        "                                                       output_attentions = False,\n",
        "                                                       output_hidden_states = False)\n",
        "\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load(bert_path, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEwTfZJ1O_IA",
        "outputId": "bc7d74bc-caa6-4ef3-8468-0d02aa5868b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader_raw):\n",
        "\n",
        "    model.eval()\n",
        "    predictions, true_vals = [], []\n",
        "    \n",
        "    for batch in dataloader_raw:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                 }\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(**inputs)\n",
        "            \n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "            \n",
        "    return predictions"
      ],
      "metadata": {
        "id": "-UpV8rVpXBw0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x, axis=1):\n",
        "    row_max = x.max(axis=axis)\n",
        "    row_max = row_max.reshape(-1, 1)\n",
        "    hatx = x - row_max\n",
        "    hatx_exp = np.exp(hatx)\n",
        "    hatx_sum = np.sum(hatx_exp, axis=axis, keepdims=True)\n",
        "    s = hatx_exp / hatx_sum\n",
        "    return s"
      ],
      "metadata": {
        "id": "Rg3GYQHqXFE4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do sentence tokenize and lower case\n",
        "df_raw_reviews['reviews'] = df_raw_reviews['reviews'].map(lambda x:sent_tokenize(x.lower()))\n",
        "df_raw_reviews = df_raw_reviews.explode('reviews')\n",
        "\n",
        "# Make data\n",
        "encoded_data_raw = tokenizer.batch_encode_plus(df_raw_reviews.reviews.values, \n",
        "                                               add_special_tokens=True, \n",
        "                                               return_attention_mask=True, \n",
        "                                               pad_to_max_length=True, \n",
        "                                               max_length=256, \n",
        "                                               return_tensors='pt'\n",
        "                                               )\n",
        "\n",
        "input_ids_raw = encoded_data_raw['input_ids']\n",
        "attention_masks_raw = encoded_data_raw['attention_mask']\n",
        "dataset_raw = TensorDataset(input_ids_raw, attention_masks_raw)\n",
        "\n",
        "batch_size = 8\n",
        "dataloader_raw = DataLoader(dataset_raw, \n",
        "                            sampler=SequentialSampler(dataset_raw), \n",
        "                            batch_size=batch_size)\n",
        "    \n",
        "# Get the labels\n",
        "predictions = evaluate(dataloader_raw)\n",
        "preds_flat = np.argmax(predictions, axis=1).flatten()     # pos: 1, neg: 2, neu: 0(we do not care about neu)\n",
        "confidence = np.max(softmax(predictions), axis=1).flatten()\n",
        "\n",
        "sentiment_raw = []\n",
        "confidence_raw = []\n",
        "for j in range(len(preds_flat)):\n",
        "    sentiment_raw.append(label_dict_inverse[preds_flat[j]])\n",
        "    confidence_raw.append(confidence[j])\n",
        "\n",
        "# Add a column of sentiment classified by BERT model trained in the last part\n",
        "df_raw_reviews.loc[:, 'sentiment'] = sentiment_raw\n",
        "df_raw_reviews.loc[:, 'confidence'] = confidence_raw\n",
        "df_raw_reviews = df_raw_reviews[df_raw_reviews['sentiment'].values != 'neu']\n",
        "df_raw_reviews = df_raw_reviews[df_raw_reviews['confidence'].values >= confidence_limit]\n",
        "\n",
        "# Reset index\n",
        "df_raw_reviews.reset_index(inplace=True)\n",
        "del df_raw_reviews['index']\n",
        "\n",
        "df_raw_reviews_pos = df_raw_reviews[df_raw_reviews['sentiment'] == 'pos']\n",
        "df_raw_reviews_neg = df_raw_reviews[df_raw_reviews['sentiment'] == 'neg']\n",
        "df_raw_reviews_pos = df_raw_reviews_pos[df_raw_reviews_pos['confidence'] >= confidence_limit]\n",
        "df_raw_reviews_neg = df_raw_reviews_neg[df_raw_reviews_neg['confidence'] >= confidence_limit]\n",
        "\n",
        "# Concatenate sentences to str\n",
        "list_pos = [item for item in df_raw_reviews_pos['reviews']]\n",
        "str_pos = ' '.join(list_pos)\n",
        "list_neg = [item for item in df_raw_reviews_neg['reviews']]\n",
        "str_neg = ' '.join(list_neg)\n",
        "\n",
        "# Save the sentences to txt\n",
        "with open(f'{save_path}/pos.txt', 'w') as f:\n",
        "    f.write(str_pos)\n",
        "    f.close()\n",
        "\n",
        "with open(f'{save_path}/neg.txt', 'w') as f:\n",
        "    f.write(str_neg)\n",
        "    f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T33zaIsdWmTb",
        "outputId": "d0b62c4c-06c8-4bc1-94ba-271bd2a4b422"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define PEGASUS\n",
        "model_name = 'google/pegasus-large'\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sum_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "sum_model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Load sentences\n",
        "with open(f'{save_path}/pos.txt') as f:\n",
        "    for eachline in f:\n",
        "        txt_pos = eachline\n",
        "\n",
        "with open(f'{save_path}/neg.txt') as f:\n",
        "    for eachline in f:\n",
        "        txt_neg = eachline"
      ],
      "metadata": {
        "id": "ENkiQ-D8bwVT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'{save_path}/sum_pos.txt', 'w') as f:\n",
        "    batch = sum_tokenizer(txt_pos, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
        "    translated = sum_model.generate(**batch)\n",
        "    sum_text = sum_tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    f.write(sum_text[0])\n",
        "\n",
        "with open(f'{save_path}/sum_neg.txt', 'w') as f:\n",
        "    batch = sum_tokenizer(txt_neg, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
        "    translated = sum_model.generate(**batch)\n",
        "    sum_text = sum_tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    f.write(sum_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjLtT_XAcCWs",
        "outputId": "9d3458c8-465d-4987-a498-fa84686fb6f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1301: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 256 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possbile that colab runs out of RAM if you run this notebook from beginning. If that happens, please restart kernel and run from here. (We have saved summarizations in the last step.)"
      ],
      "metadata": {
        "id": "HvQmx2Ktv7MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This should be the same as the beginning\n",
        "\n",
        "# If you predownload SUM-QE models, please also enter the path of the models here\n",
        "sum_qe_path = ''\n",
        "\n",
        "# After preprocessing the reviews, we save the classified sentences (positive and negative) here\n",
        "# Summarization is also saved here\n",
        "save_path = '/content/drive/MyDrive/GR/classified review'"
      ],
      "metadata": {
        "id": "LDVPhhz6wmgm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/SumQE')\n",
        "%cd /content/SumQE/\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# src/examples.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models import load_model\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from src.BERT_experiments.BERT_model import BERT, custom_loss, set_quality_index\n",
        "from src.vectorizer import BERTVectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL8j8eQrloM-",
        "outputId": "c6bb01bf-77b7-4588-91ba-31e5488e67c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SumQE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn scores to \"good\", \"fair\", and \"bad\"\n",
        "# The value comes from the corresponding paper\n",
        "def measure_quality(name, score, sentiment):\n",
        "    if name == 1 and sentiment == 'pos':\n",
        "        if score < 0.2504:\n",
        "            quality = 'bad'\n",
        "        if score > 0.3704:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "\n",
        "    if name == 2 and sentiment == 'pos':\n",
        "        if score < 0.2543:\n",
        "            quality = 'bad'\n",
        "        if score > 0.3737:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "\n",
        "    if name == 3 and sentiment == 'pos':\n",
        "        if score < 0.2515:\n",
        "            quality = 'bad'\n",
        "        if score > 0.371:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "\n",
        "    if name == 4 and sentiment == 'pos':\n",
        "        if score < 0.1044:\n",
        "            quality = 'bad'\n",
        "        if score > 0.2692:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "\n",
        "    if name == 5 and sentiment == 'pos':\n",
        "        if score < 0.1111:\n",
        "            quality = 'bad'\n",
        "        if score > 0.2737:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "\n",
        "    if name == 1 and sentiment == 'neg':\n",
        "        if score < 0.3597:\n",
        "            quality = 'bad'\n",
        "        if score > 0.5103:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "\n",
        "    if name == 2 and sentiment == 'neg':\n",
        "        if score < 0.3611:\n",
        "            quality = 'bad'\n",
        "        if score > 0.5111:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "\n",
        "    if name == 3 and sentiment == 'neg':\n",
        "        if score < 0.358:\n",
        "            quality = 'bad'\n",
        "        if score > 0.5089:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "\n",
        "    if name == 4 and sentiment == 'neg':\n",
        "        if score < 0.2423:\n",
        "            quality = 'bad'\n",
        "        if score > 0.4681:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "\n",
        "    if name == 5 and sentiment == 'neg':\n",
        "        if score < 0.2449:\n",
        "            quality = 'bad'\n",
        "        if score > 0.4707:\n",
        "            quality = 'good'\n",
        "        else:\n",
        "            quality = 'fair'\n",
        "    return quality"
      ],
      "metadata": {
        "id": "jRagKFuNnXom"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quality evaluation\n",
        "scores = []\n",
        "Qs = []\n",
        "senti = []\n",
        "quality = []\n",
        "sentiment_range = ['pos', 'neg']\n",
        "for i in range(1, 6):\n",
        "    print('[INFO] Running for Q{}'.format(i))\n",
        "    QUALITY = 'Q{}'.format(i)\n",
        "    model_path = '/content/BERT_DUC_all_{}_Multi Task-1.h5'.format(QUALITY)\n",
        "\n",
        "    # Set the quality index used in custom_loss\n",
        "    set_quality_index(mode = 'Multi Task-1', quality = QUALITY)\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model(model_path, custom_objects={'BERT': BERT, 'custom_loss': custom_loss})\n",
        "\n",
        "    # Define the vectorizer\n",
        "    vectorizer = BERTVectorizer()\n",
        "\n",
        "    for j in range(2):\n",
        "        sentiment = sentiment_range[j]\n",
        "        # For each model, do scoring\n",
        "        Qs.append('Q{}'.format(i))\n",
        "        senti.append(sentiment)\n",
        "        with open(f'{save_path}/sum_{sentiment}.txt') as f:\n",
        "            for eachline in f:\n",
        "                after_sum = eachline\n",
        "\n",
        "        # Summarization\n",
        "        summary_token_ids = []\n",
        "        for k, sentence in enumerate(sent_tokenize(after_sum)):\n",
        "            sentence_tok = vectorizer.vectorize_inputs(sequence = sentence, i = k)\n",
        "            summary_token_ids = summary_token_ids + sentence_tok\n",
        "                \n",
        "        # Transform the summary_tokens_ids into inputs --> (bpe_ids, mask, segments)\n",
        "        inputs = vectorizer.transform_to_inputs(summary_token_ids)\n",
        "\n",
        "        # Construct the dict that you will feed on your network. If you have multiple summaries,\n",
        "        # you can update the lists and feed all of them together.\n",
        "        test_dict = {\n",
        "            'word_inputs': np.asarray([inputs[0, 0]]),\n",
        "            'pos_inputs': np.asarray([inputs[1, 0]]),\n",
        "            'seg_inputs': np.asarray([inputs[2, 0]])\n",
        "        }\n",
        "              \n",
        "        output = model.predict(test_dict, batch_size=1)\n",
        "        quality_score = round(output[0][0], 4)\n",
        "        scores.append(quality_score)\n",
        "        quality.append(measure_quality(i, quality_score, sentiment))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIQMJ_0KlaCl",
        "outputId": "962125f1-5d5f-4b96-999c-e4e0ee0a5eef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running for Q1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/SumQE/src/BERT_experiments/tokenization.py:32: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running for Q2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running for Q3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running for Q4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running for Q5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df = pd.DataFrame({'QE score':scores,'measurement':Qs, 'sentiment':senti, 'quality': quality})\n",
        "scores_df.to_csv('/content/drive/MyDrive/GR/scores_test.csv', sep = ',', index = True)\n",
        "scores_df = scores_df.sort_values(by=[\"sentiment\", \"measurement\"], ascending=[False, True])"
      ],
      "metadata": {
        "id": "9NzT5nxE6xFF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the results!"
      ],
      "metadata": {
        "id": "izU_njj2Awqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'{save_path}/sum_pos.txt') as f:\n",
        "            for eachline in f:\n",
        "                sum_pos = eachline\n",
        "\n",
        "with open(f'{save_path}/sum_neg.txt') as f:\n",
        "            for eachline in f:\n",
        "                sum_neg = eachline"
      ],
      "metadata": {
        "id": "zufP4D5sAwHk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "UuEzTNgiBAPk",
        "outputId": "22dc6363-1b13-4db1-d7f9-7193b5b7b6a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i'm glad to say that i did find a great dryer, the conair infinitipro 1875 (the cheap rust orange one) and this one does dry my hair rapidly and i have far less frizz with it. it cost me less than half of what i originally spent on the trezoro (i got it for $12.99 at ross so it was even cheaper than it goes on for here) and it performs like a high end salon dryer. i have very fine and thin hair..with that in mind the dryer has only 2 speeds 'high' and'medium high' so, it blows my hair everywhere and because it's fine it tangles easier.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum_neg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0jPcGa-UBEfB",
        "outputId": "1a5659be-89da-419d-ccb4-e157a355ca4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the plug for the hairdryer had an electric issue and short wired cause sparks to fly and for my hair dryer to break.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "fMUafix16yUa",
        "outputId": "d051269b-c8de-4697-81a4-6e0b98599f91"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   QE score measurement sentiment quality\n",
              "0    0.4939          Q1       pos    good\n",
              "2    0.4968          Q2       pos    good\n",
              "4    0.4955          Q3       pos    good\n",
              "6    0.4362          Q4       pos    good\n",
              "8    0.4424          Q5       pos    good\n",
              "1    0.4741          Q1       neg    fair\n",
              "3    0.4759          Q2       neg    fair\n",
              "5    0.4739          Q3       neg    fair\n",
              "7    0.4910          Q4       neg    good\n",
              "9    0.4935          Q5       neg    good"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc391188-2351-479b-bbc4-8c833bbef08c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QE score</th>\n",
              "      <th>measurement</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.4939</td>\n",
              "      <td>Q1</td>\n",
              "      <td>pos</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.4968</td>\n",
              "      <td>Q2</td>\n",
              "      <td>pos</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.4955</td>\n",
              "      <td>Q3</td>\n",
              "      <td>pos</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.4362</td>\n",
              "      <td>Q4</td>\n",
              "      <td>pos</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.4424</td>\n",
              "      <td>Q5</td>\n",
              "      <td>pos</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.4741</td>\n",
              "      <td>Q1</td>\n",
              "      <td>neg</td>\n",
              "      <td>fair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.4759</td>\n",
              "      <td>Q2</td>\n",
              "      <td>neg</td>\n",
              "      <td>fair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.4739</td>\n",
              "      <td>Q3</td>\n",
              "      <td>neg</td>\n",
              "      <td>fair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.4910</td>\n",
              "      <td>Q4</td>\n",
              "      <td>neg</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.4935</td>\n",
              "      <td>Q5</td>\n",
              "      <td>neg</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc391188-2351-479b-bbc4-8c833bbef08c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dc391188-2351-479b-bbc4-8c833bbef08c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dc391188-2351-479b-bbc4-8c833bbef08c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}